2021-06-28 10:11:33.616451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2021-06-28 10:11:33.619576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
/root/ModelGarden/models-r2.1.0/official/modeling/hyperparams/params_dict.py:402: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  params_dict = yaml.load(dict_or_string_or_yaml_file)
I0628 10:11:35.141472 139676278892352 main.py:195] Model Parameters: {'anchor': {'anchor_size': 4.0,
            'aspect_ratios': [1.0, 2.0, 0.5],
            'max_level': 7,
            'min_level': 3,
            'num_scales': 3},
 'architecture': {'backbone': 'resnet',
                  'multilevel_features': 'fpn',
                  'parser': 'retinanet_parser',
                  'use_bfloat16': False},
 'enable_summary': False,
 'eval': {'batch_size': 8,
          'eval_file_pattern': 'gs://apiss/coco/val-*',
          'eval_samples': 5000,
          'eval_timeout': None,
          'input_sharding': True,
          'min_eval_interval': 180,
          'type': 'box',
          'val_json_file': 'gs://apiss/coco/instances_val2017.json'},
 'fpn': {'batch_norm': {'batch_norm_epsilon': 0.0001,
                        'batch_norm_momentum': 0.997,
                        'batch_norm_trainable': True},
         'fpn_feat_dims': 256,
         'max_level': 7,
         'min_level': 3,
         'use_separable_conv': False},
 'model_dir': 'gs://apiss/coco/Model/cocopy_func_new13',
 'nasfpn': {'batch_norm': {'batch_norm_epsilon': 0.0001,
                           'batch_norm_momentum': 0.997,
                           'batch_norm_trainable': True},
            'dropblock': {'dropblock_keep_prob': None, 'dropblock_size': None},
            'fpn_feat_dims': 256,
            'max_level': 7,
            'min_level': 3,
            'num_repeats': 5,
            'use_separable_conv': False},
 'postprocess': {'max_level': 7,
                 'max_total_size': 100,
                 'min_level': 3,
                 'nms_iou_threshold': 0.5,
                 'pre_nms_num_boxes': 5000,
                 'score_threshold': 0.05,
                 'use_batched_nms': False},
 'predict': {'predict_batch_size': 8},
 'resnet': {'batch_norm': {'batch_norm_epsilon': 0.0001,
                           'batch_norm_momentum': 0.997,
                           'batch_norm_trainable': True},
            'dropblock': {'dropblock_keep_prob': None, 'dropblock_size': None},
            'resnet_depth': 50},
 'retinanet_head': {'anchors_per_location': 9,
                    'batch_norm': {'batch_norm_epsilon': 0.0001,
                                   'batch_norm_momentum': 0.997,
                                   'batch_norm_trainable': True},
                    'max_level': 7,
                    'min_level': 3,
                    'num_classes': 91,
                    'retinanet_head_num_convs': 4,
                    'retinanet_head_num_filters': 256,
                    'use_separable_conv': False},
 'retinanet_loss': {'box_loss_weight': 50,
                    'focal_loss_alpha': 0.25,
                    'focal_loss_gamma': 1.5,
                    'huber_loss_delta': 0.1,
                    'num_classes': 91},
 'retinanet_parser': {'aug_rand_hflip': True,
                      'aug_scale_max': 1.0,
                      'aug_scale_min': 1.0,
                      'autoaugment_policy_name': 'v0',
                      'match_threshold': 0.5,
                      'max_num_instances': 100,
                      'num_channels': 3,
                      'output_size': [640, 640],
                      'skip_crowd_during_training': True,
                      'unmatched_threshold': 0.5,
                      'use_autoaugment': False,
                      'use_bfloat16': False},
 'strategy_config': {'all_reduce_alg': None,
                     'num_gpus': 0,
                     'num_packs': 1,
                     'task_index': -1,
                     'tpu': 'node-1',
                     'worker_hosts': None},
 'strategy_type': 'tpu',
 'train': {'batch_size': 64,
           'checkpoint': {'path': 'gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07',
                          'prefix': 'resnet50/'},
           'frozen_variable_prefix': '(resnet\\d+/)conv2d(|_([1-9]|10))\\/',
           'input_sharding': False,
           'iterations_per_loop': 500,
           'l2_weight_decay': 0.0001,
           'learning_rate': {'init_learning_rate': 0.08,
                             'learning_rate_levels': [0.008, 0.0008],
                             'learning_rate_steps': [15000, 20000],
                             'type': 'step',
                             'warmup_learning_rate': 0.0067,
                             'warmup_steps': 500},
           'optimizer': {'momentum': 0.9, 'nesterov': True, 'type': 'momentum'},
           'total_steps': 5000,
           'train_file_pattern': 'gs://apiss/coco/train-*',
           'transpose_input': False},
 'type': 'retinanet',
 'use_tpu': True}
I0628 10:11:35.321603 139676278892352 transport.py:157] Attempting refresh to obtain initial access_token
I0628 10:11:35.321953 139676278892352 client.py:777] Refreshing access_token
I0628 10:11:35.459691 139676278892352 transport.py:157] Attempting refresh to obtain initial access_token
I0628 10:11:35.459986 139676278892352 client.py:777] Refreshing access_token
2021-06-28 10:11:35.567954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-28 10:11:35.568006: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (-1)
2021-06-28 10:11:35.568061: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (4772c6b3998a): /proc/driver/nvidia/version does not exist
I0628 10:11:35.571918 139676278892352 transport.py:157] Attempting refresh to obtain initial access_token
I0628 10:11:35.572153 139676278892352 client.py:777] Refreshing access_token
I0628 10:11:35.686895 139676278892352 transport.py:157] Attempting refresh to obtain initial access_token
I0628 10:11:35.687145 139676278892352 client.py:777] Refreshing access_token
I0628 10:11:35.811418 139676278892352 remote.py:177] Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0
2021-06-28 10:11:35.812029: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-06-28 10:11:35.824744: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz
2021-06-28 10:11:35.825228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45fcc00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-28 10:11:35.825305: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-28 10:11:35.846221: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> 10.111.230.146:8470}
2021-06-28 10:11:35.846278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:36185}
2021-06-28 10:11:35.863441: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> 10.111.230.146:8470}
2021-06-28 10:11:35.863494: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:36185}
2021-06-28 10:11:35.864363: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:36185
INFO:tensorflow:Initializing the TPU system: node-1
I0628 10:11:35.865102 139676278892352 tpu_strategy_util.py:72] Initializing the TPU system: node-1
INFO:tensorflow:Clearing out eager caches
I0628 10:11:36.049603 139676278892352 tpu_strategy_util.py:100] Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
I0628 10:11:43.899589 139676278892352 tpu_strategy_util.py:123] Finished initializing TPU system.
I0628 10:11:43.905355 139676278892352 transport.py:157] Attempting refresh to obtain initial access_token
I0628 10:11:43.905597 139676278892352 client.py:777] Refreshing access_token
I0628 10:11:44.027096 139676278892352 transport.py:157] Attempting refresh to obtain initial access_token
I0628 10:11:44.027488 139676278892352 client.py:777] Refreshing access_token
INFO:tensorflow:Found TPU system:
I0628 10:11:44.136771 139676278892352 tpu_system_metadata.py:140] Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
I0628 10:11:44.137047 139676278892352 tpu_system_metadata.py:141] *** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
I0628 10:11:44.137179 139676278892352 tpu_system_metadata.py:142] *** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
I0628 10:11:44.137274 139676278892352 tpu_system_metadata.py:144] *** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
I0628 10:11:44.137393 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0628 10:11:44.138406 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
I0628 10:11:44.138552 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
I0628 10:11:44.138678 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
I0628 10:11:44.138804 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
I0628 10:11:44.138936 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
I0628 10:11:44.139047 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
I0628 10:11:44.139165 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
I0628 10:11:44.139284 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
I0628 10:11:44.139408 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
I0628 10:11:44.139527 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
I0628 10:11:44.139625 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0628 10:11:44.139720 139676278892352 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0628 10:11:44.143321 139676278892352 main.py:93] Train num_replicas_in_sync 8 num_workers 1 is_multi_host False
I0628 10:11:44.143520 139676278892352 distributed_executor.py:145] Save config to model_dir gs://apiss/coco/Model/cocopy_func_new13.
WARNING:tensorflow:AutoGraph could not transform <function InputFn.__call__.<locals>.<lambda> at 0x7f083c53b048> and will run it as-is.
Cause: Unable to identify source code of lambda function <function InputFn.__call__.<locals>.<lambda> at 0x7f083c53b048>. It was defined on this line: from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
map_func, which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.
W0628 10:11:46.479553 139676278892352 ag_logging.py:146] AutoGraph could not transform <function InputFn.__call__.<locals>.<lambda> at 0x7f083c53b048> and will run it as-is.
Cause: Unable to identify source code of lambda function <function InputFn.__call__.<locals>.<lambda> at 0x7f083c53b048>. It was defined on this line: from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
map_func, which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.
I0628 10:12:27.335421 139676278892352 distributed_executor.py:382] Checkpoint file gs://apiss/coco/Model/cocopy_func_new13/ctl_step_2000.ckpt-4 found and restoring from checkpoint
I0628 10:12:33.582806 139676278892352 distributed_executor.py:386] Loading from checkpoint file completed. Init step 2000
I0628 10:12:33.613911 139676278892352 detection_executor.py:60] Filter trainable variables from 285 to 274
E0628 10:12:33.614123 139676278892352 detection_executor.py:66] Detection: train metric is not an instance of tf.keras.metrics.Metric.
I0628 10:12:33.614457 139676278892352 distributed_executor.py:411] Training started
I0628 10:16:42.109126 139676278892352 distributed_executor.py:446] Train Step: 2500/5000  / loss = {'model_loss': 0.09291490912437439, 'total_loss': 0.18580149114131927, 'box_loss': 0.0006120029138401151, 'l2_regularization_loss': 0.09288659691810608, 'cls_loss': 0.06231475621461868, 'learning_rate': 0.08} / training metric = {'model_loss': 0.09291490912437439, 'total_loss': 0.18580149114131927, 'box_loss': 0.0006120029138401151, 'l2_regularization_loss': 0.09288659691810608, 'cls_loss': 0.06231475621461868, 'learning_rate': 0.08}
I0628 10:16:52.701863 139676278892352 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new13/ctl_step_2500.ckpt-5
I0628 10:18:40.324809 139676278892352 distributed_executor.py:446] Train Step: 3000/5000  / loss = {'model_loss': 0.0911329984664917, 'total_loss': 0.1783970445394516, 'box_loss': 0.0005416281637735665, 'l2_regularization_loss': 0.0872640460729599, 'cls_loss': 0.06405159085988998, 'learning_rate': 0.08} / training metric = {'model_loss': 0.0911329984664917, 'total_loss': 0.1783970445394516, 'box_loss': 0.0005416281637735665, 'l2_regularization_loss': 0.0872640460729599, 'cls_loss': 0.06405159085988998, 'learning_rate': 0.08}
I0628 10:18:49.943632 139676278892352 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new13/ctl_step_3000.ckpt-6
I0628 10:20:37.264099 139676278892352 distributed_executor.py:446] Train Step: 3500/5000  / loss = {'model_loss': 0.08617222309112549, 'total_loss': 0.16822725534439087, 'box_loss': 0.0005772089934907854, 'l2_regularization_loss': 0.08205502480268478, 'cls_loss': 0.0573117733001709, 'learning_rate': 0.08} / training metric = {'model_loss': 0.08617222309112549, 'total_loss': 0.16822725534439087, 'box_loss': 0.0005772089934907854, 'l2_regularization_loss': 0.08205502480268478, 'cls_loss': 0.0573117733001709, 'learning_rate': 0.08}
I0628 10:20:47.806406 139676278892352 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new13/ctl_step_3500.ckpt-7
I0628 10:22:34.893238 139676278892352 distributed_executor.py:446] Train Step: 4000/5000  / loss = {'model_loss': 0.08232071995735168, 'total_loss': 0.15958723425865173, 'box_loss': 0.0005410201847553253, 'l2_regularization_loss': 0.07726650685071945, 'cls_loss': 0.05526971444487572, 'learning_rate': 0.08} / training metric = {'model_loss': 0.08232071995735168, 'total_loss': 0.15958723425865173, 'box_loss': 0.0005410201847553253, 'l2_regularization_loss': 0.07726650685071945, 'cls_loss': 0.05526971444487572, 'learning_rate': 0.08}
I0628 10:22:44.152807 139676278892352 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new13/ctl_step_4000.ckpt-8
I0628 10:24:31.150732 139676278892352 distributed_executor.py:446] Train Step: 4500/5000  / loss = {'model_loss': 0.07861826568841934, 'total_loss': 0.1515813022851944, 'box_loss': 0.0005274720024317503, 'l2_regularization_loss': 0.07296303659677505, 'cls_loss': 0.05224466323852539, 'learning_rate': 0.08} / training metric = {'model_loss': 0.07861826568841934, 'total_loss': 0.1515813022851944, 'box_loss': 0.0005274720024317503, 'l2_regularization_loss': 0.07296303659677505, 'cls_loss': 0.05224466323852539, 'learning_rate': 0.08}
I0628 10:24:41.666198 139676278892352 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new13/ctl_step_4500.ckpt-9
I0628 10:26:28.835647 139676278892352 distributed_executor.py:446] Train Step: 5000/5000  / loss = {'model_loss': 0.08645952492952347, 'total_loss': 0.15545624494552612, 'box_loss': 0.0005969334160909057, 'l2_regularization_loss': 0.06899672001600266, 'cls_loss': 0.056612852960824966, 'learning_rate': 0.08} / training metric = {'model_loss': 0.08645952492952347, 'total_loss': 0.15545624494552612, 'box_loss': 0.0005969334160909057, 'l2_regularization_loss': 0.06899672001600266, 'cls_loss': 0.056612852960824966, 'learning_rate': 0.08}
I0628 10:26:38.094186 139676278892352 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new13/ctl_step_5000.ckpt-10
2021-06-28 10:26:41.263354: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:79] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find a context_id matching the specified one (3150766149033781736). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{"created":"@1624890401.262814964","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Unable to find a context_id matching the specified one (3150766149033781736). Perhaps the worker was restarted, or the context was GC'd?","grpc_status":3}

2021-06-27 23:54:42.914451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2021-06-27 23:54:42.917569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
/root/ModelGarden/models-r2.1.0/official/modeling/hyperparams/params_dict.py:402: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  params_dict = yaml.load(dict_or_string_or_yaml_file)
I0627 23:54:44.346571 140408690603840 main.py:195] Model Parameters: {'anchor': {'anchor_size': 4.0,
            'aspect_ratios': [1.0, 2.0, 0.5],
            'max_level': 7,
            'min_level': 3,
            'num_scales': 3},
 'architecture': {'backbone': 'resnet',
                  'multilevel_features': 'fpn',
                  'parser': 'retinanet_parser',
                  'use_bfloat16': False},
 'enable_summary': False,
 'eval': {'batch_size': 8,
          'eval_file_pattern': 'gs://apiss/coco/val-*',
          'eval_samples': 5000,
          'eval_timeout': None,
          'input_sharding': True,
          'min_eval_interval': 180,
          'type': 'box',
          'val_json_file': 'gs://apiss/coco/instances_val2017.json'},
 'fpn': {'batch_norm': {'batch_norm_epsilon': 0.0001,
                        'batch_norm_momentum': 0.997,
                        'batch_norm_trainable': True},
         'fpn_feat_dims': 256,
         'max_level': 7,
         'min_level': 3,
         'use_separable_conv': False},
 'model_dir': 'gs://apiss/coco/Model/cocopy_func_new9',
 'nasfpn': {'batch_norm': {'batch_norm_epsilon': 0.0001,
                           'batch_norm_momentum': 0.997,
                           'batch_norm_trainable': True},
            'dropblock': {'dropblock_keep_prob': None, 'dropblock_size': None},
            'fpn_feat_dims': 256,
            'max_level': 7,
            'min_level': 3,
            'num_repeats': 5,
            'use_separable_conv': False},
 'postprocess': {'max_level': 7,
                 'max_total_size': 100,
                 'min_level': 3,
                 'nms_iou_threshold': 0.5,
                 'pre_nms_num_boxes': 5000,
                 'score_threshold': 0.05,
                 'use_batched_nms': False},
 'predict': {'predict_batch_size': 8},
 'resnet': {'batch_norm': {'batch_norm_epsilon': 0.0001,
                           'batch_norm_momentum': 0.997,
                           'batch_norm_trainable': True},
            'dropblock': {'dropblock_keep_prob': None, 'dropblock_size': None},
            'resnet_depth': 50},
 'retinanet_head': {'anchors_per_location': 9,
                    'batch_norm': {'batch_norm_epsilon': 0.0001,
                                   'batch_norm_momentum': 0.997,
                                   'batch_norm_trainable': True},
                    'max_level': 7,
                    'min_level': 3,
                    'num_classes': 91,
                    'retinanet_head_num_convs': 4,
                    'retinanet_head_num_filters': 256,
                    'use_separable_conv': False},
 'retinanet_loss': {'box_loss_weight': 50,
                    'focal_loss_alpha': 0.25,
                    'focal_loss_gamma': 1.5,
                    'huber_loss_delta': 0.1,
                    'num_classes': 91},
 'retinanet_parser': {'aug_rand_hflip': True,
                      'aug_scale_max': 1.0,
                      'aug_scale_min': 1.0,
                      'autoaugment_policy_name': 'v0',
                      'match_threshold': 0.5,
                      'max_num_instances': 100,
                      'num_channels': 3,
                      'output_size': [640, 640],
                      'skip_crowd_during_training': True,
                      'unmatched_threshold': 0.5,
                      'use_autoaugment': False,
                      'use_bfloat16': False},
 'strategy_config': {'all_reduce_alg': None,
                     'num_gpus': 0,
                     'num_packs': 1,
                     'task_index': -1,
                     'tpu': 'node-1',
                     'worker_hosts': None},
 'strategy_type': 'tpu',
 'train': {'batch_size': 64,
           'checkpoint': {'path': 'gs://cloud-tpu-checkpoints/retinanet/resnet50-checkpoint-2018-02-07',
                          'prefix': 'resnet50/'},
           'frozen_variable_prefix': '(resnet\\d+/)conv2d(|_([1-9]|10))\\/',
           'input_sharding': False,
           'iterations_per_loop': 500,
           'l2_weight_decay': 0.0001,
           'learning_rate': {'init_learning_rate': 0.08,
                             'learning_rate_levels': [0.008, 0.0008],
                             'learning_rate_steps': [15000, 20000],
                             'type': 'step',
                             'warmup_learning_rate': 0.0067,
                             'warmup_steps': 500},
           'optimizer': {'momentum': 0.9, 'nesterov': True, 'type': 'momentum'},
           'total_steps': 5000,
           'train_file_pattern': 'gs://apiss/coco/train-*',
           'transpose_input': False},
 'type': 'retinanet',
 'use_tpu': True}
I0627 23:54:44.521691 140408690603840 transport.py:157] Attempting refresh to obtain initial access_token
I0627 23:54:44.522017 140408690603840 client.py:777] Refreshing access_token
I0627 23:54:44.677347 140408690603840 transport.py:157] Attempting refresh to obtain initial access_token
I0627 23:54:44.677580 140408690603840 client.py:777] Refreshing access_token
2021-06-27 23:54:44.790435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-27 23:54:44.790489: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (-1)
2021-06-27 23:54:44.790533: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (4772c6b3998a): /proc/driver/nvidia/version does not exist
I0627 23:54:44.794402 140408690603840 transport.py:157] Attempting refresh to obtain initial access_token
I0627 23:54:44.794624 140408690603840 client.py:777] Refreshing access_token
I0627 23:54:44.920922 140408690603840 transport.py:157] Attempting refresh to obtain initial access_token
I0627 23:54:44.921206 140408690603840 client.py:777] Refreshing access_token
I0627 23:54:45.017651 140408690603840 remote.py:177] Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0
2021-06-27 23:54:45.018087: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-06-27 23:54:45.028148: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz
2021-06-27 23:54:45.028774: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x46c3420 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-27 23:54:45.028820: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-27 23:54:45.048541: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> 10.111.230.146:8470}
2021-06-27 23:54:45.048600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:35251}
2021-06-27 23:54:45.066050: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> 10.111.230.146:8470}
2021-06-27 23:54:45.066112: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:35251}
2021-06-27 23:54:45.066902: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:35251
INFO:tensorflow:Initializing the TPU system: node-1
I0627 23:54:45.068056 140408690603840 tpu_strategy_util.py:72] Initializing the TPU system: node-1
INFO:tensorflow:Clearing out eager caches
I0627 23:54:45.244934 140408690603840 tpu_strategy_util.py:100] Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
I0627 23:54:54.568613 140408690603840 tpu_strategy_util.py:123] Finished initializing TPU system.
I0627 23:54:54.573712 140408690603840 transport.py:157] Attempting refresh to obtain initial access_token
I0627 23:54:54.573944 140408690603840 client.py:777] Refreshing access_token
I0627 23:54:54.682885 140408690603840 transport.py:157] Attempting refresh to obtain initial access_token
I0627 23:54:54.683136 140408690603840 client.py:777] Refreshing access_token
INFO:tensorflow:Found TPU system:
I0627 23:54:54.789890 140408690603840 tpu_system_metadata.py:140] Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
I0627 23:54:54.790111 140408690603840 tpu_system_metadata.py:141] *** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
I0627 23:54:54.790237 140408690603840 tpu_system_metadata.py:142] *** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
I0627 23:54:54.790348 140408690603840 tpu_system_metadata.py:144] *** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
I0627 23:54:54.790458 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0627 23:54:54.791432 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
I0627 23:54:54.791577 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
I0627 23:54:54.791718 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
I0627 23:54:54.791876 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
I0627 23:54:54.792004 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
I0627 23:54:54.792131 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
I0627 23:54:54.792263 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
I0627 23:54:54.792368 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
I0627 23:54:54.792463 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
I0627 23:54:54.792555 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
I0627 23:54:54.792650 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0627 23:54:54.792756 140408690603840 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
I0627 23:54:54.795697 140408690603840 main.py:93] Train num_replicas_in_sync 8 num_workers 1 is_multi_host False
I0627 23:54:54.795912 140408690603840 distributed_executor.py:145] Save config to model_dir gs://apiss/coco/Model/cocopy_func_new9.
WARNING:tensorflow:AutoGraph could not transform <function InputFn.__call__.<locals>.<lambda> at 0x7fb2c477a048> and will run it as-is.
Cause: Unable to identify source code of lambda function <function InputFn.__call__.<locals>.<lambda> at 0x7fb2c477a048>. It was defined on this line: from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
map_func, which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.
W0627 23:54:58.218412 140408690603840 ag_logging.py:146] AutoGraph could not transform <function InputFn.__call__.<locals>.<lambda> at 0x7fb2c477a048> and will run it as-is.
Cause: Unable to identify source code of lambda function <function InputFn.__call__.<locals>.<lambda> at 0x7fb2c477a048>. It was defined on this line: from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
map_func, which must contain a single lambda with matching signature. To avoid ambiguity, define each lambda in a separate expression.
I0627 23:55:39.430408 140408690603840 distributed_executor.py:382] Checkpoint file gs://apiss/coco/Model/cocopy_func_new9/ctl_step_2500.ckpt-5 found and restoring from checkpoint
I0627 23:55:44.729070 140408690603840 distributed_executor.py:386] Loading from checkpoint file completed. Init step 2500
I0627 23:55:44.760761 140408690603840 detection_executor.py:60] Filter trainable variables from 285 to 274
E0627 23:55:44.760979 140408690603840 detection_executor.py:66] Detection: train metric is not an instance of tf.keras.metrics.Metric.
I0627 23:55:44.761347 140408690603840 distributed_executor.py:411] Training started
I0627 23:59:54.029759 140408690603840 distributed_executor.py:446] Train Step: 3000/5000  / loss = {'cls_loss': 0.06138884648680687, 'box_loss': 0.0005662083858624101, 'total_loss': 0.1769808530807495, 'model_loss': 0.08969926834106445, 'l2_regularization_loss': 0.08728158473968506, 'learning_rate': 0.08} / training metric = {'cls_loss': 0.06138884648680687, 'box_loss': 0.0005662083858624101, 'total_loss': 0.1769808530807495, 'model_loss': 0.08969926834106445, 'l2_regularization_loss': 0.08728158473968506, 'learning_rate': 0.08}
I0628 00:00:03.420578 140408690603840 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new9/ctl_step_3000.ckpt-6
I0628 00:01:51.069073 140408690603840 distributed_executor.py:446] Train Step: 3500/5000  / loss = {'cls_loss': 0.050814658403396606, 'box_loss': 0.0005366269033402205, 'total_loss': 0.15975460410118103, 'model_loss': 0.07764600217342377, 'l2_regularization_loss': 0.08210859447717667, 'learning_rate': 0.08} / training metric = {'cls_loss': 0.050814658403396606, 'box_loss': 0.0005366269033402205, 'total_loss': 0.15975460410118103, 'model_loss': 0.07764600217342377, 'l2_regularization_loss': 0.08210859447717667, 'learning_rate': 0.08}
I0628 00:02:00.108876 140408690603840 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new9/ctl_step_3500.ckpt-7
I0628 00:03:47.596334 140408690603840 distributed_executor.py:446] Train Step: 4000/5000  / loss = {'cls_loss': 0.0549507737159729, 'box_loss': 0.0005197154823690653, 'total_loss': 0.15826576948165894, 'model_loss': 0.08093655109405518, 'l2_regularization_loss': 0.07732921838760376, 'learning_rate': 0.08} / training metric = {'cls_loss': 0.0549507737159729, 'box_loss': 0.0005197154823690653, 'total_loss': 0.15826576948165894, 'model_loss': 0.08093655109405518, 'l2_regularization_loss': 0.07732921838760376, 'learning_rate': 0.08}
I0628 00:03:56.628773 140408690603840 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new9/ctl_step_4000.ckpt-8
I0628 00:05:44.583053 140408690603840 distributed_executor.py:446] Train Step: 4500/5000  / loss = {'cls_loss': 0.05422356724739075, 'box_loss': 0.000478452566312626, 'total_loss': 0.15111353993415833, 'model_loss': 0.07814619690179825, 'l2_regularization_loss': 0.07296734303236008, 'learning_rate': 0.08} / training metric = {'cls_loss': 0.05422356724739075, 'box_loss': 0.000478452566312626, 'total_loss': 0.15111353993415833, 'model_loss': 0.07814619690179825, 'l2_regularization_loss': 0.07296734303236008, 'learning_rate': 0.08}
I0628 00:05:53.701850 140408690603840 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new9/ctl_step_4500.ckpt-9
I0628 00:07:41.032784 140408690603840 distributed_executor.py:446] Train Step: 5000/5000  / loss = {'cls_loss': 0.05190804600715637, 'box_loss': 0.0005413167527876794, 'total_loss': 0.14799444377422333, 'model_loss': 0.07897388935089111, 'l2_regularization_loss': 0.06902056932449341, 'learning_rate': 0.08} / training metric = {'cls_loss': 0.05190804600715637, 'box_loss': 0.0005413167527876794, 'total_loss': 0.14799444377422333, 'model_loss': 0.07897388935089111, 'l2_regularization_loss': 0.06902056932449341, 'learning_rate': 0.08}
I0628 00:07:50.215212 140408690603840 distributed_executor.py:49] Saving model as TF checkpoint: gs://apiss/coco/Model/cocopy_func_new9/ctl_step_5000.ckpt-10
2021-06-28 00:07:53.414337: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:79] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find a context_id matching the specified one (8278446270021328119). Perhaps the worker was restarted, or the context was GC'd?
Additional GRPC error information:
{"created":"@1624853273.413806513","description":"Error received from peer","file":"external/grpc/src/core/lib/surface/call.cc","file_line":1039,"grpc_message":"Unable to find a context_id matching the specified one (8278446270021328119). Perhaps the worker was restarted, or the context was GC'd?","grpc_status":3}
